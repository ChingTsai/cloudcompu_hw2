import org.apache.spark._
import org.apache.hadoop.fs._

object PageRankSp {
  def main(args: Array[String]) {
    val filePath = args(0)
    val name = args(1)
    val iters = args(2).toInt

    val outputPath = "Hw2/pageranksp"

    val conf = new SparkConf().setAppName("Page Rank Spark")
    val sc = new SparkContext(conf)

    // Cleanup output dir
    val hadoopConf = sc.hadoopConfiguration
    var hdfs = FileSystem.get(hadoopConf)
    try { hdfs.delete(new Path(outputPath), true) } catch { case _: Throwable => {} }

    // Read input file
    def f(s: String) = List(s.split(",")(1) -> s.split(",")(0), s.split(",")(0) -> s.split(",")(1));
    
    val lines = sc.textFile(filePath, sc.defaultParallelism)
    var map = lines.flatMap(x => f(x));

    var res = List(name);
    var rdd = sc.parallelize(res,sc.defaultParallelism*3);

    /*
    for (i <- 1 to iters) {
      val tpl = rdd.map { x => (x,x) };
      val tmp_map = tpl.join(map);
      rdd = rdd ++ tmp_map.values.values;
      rdd = rdd.distinct();
      //val friend = map.filter(x => rdd.collect().contains(x._1)).values.distinct();
      
      
    }

    // Action branch! Add cache() to avoid re-computation
    //res = res.cache
    rdd =  rdd.cache();
    // Output: count() and saveAsTextFile()
    //println("Count := " + res.count)
     
     */
    rdd.sortBy(_.toString).saveAsTextFile(outputPath)

    sc.stop
  }
}

