import org.apache.spark._
import org.apache.hadoop.fs._

object PageRankSp {
  def main(args: Array[String]) {
    val filePath = args(0)

    val outputPath = "Hw2/pageranksp"

    val conf = new SparkConf().setAppName("Page Rank Spark")
    val sc = new SparkContext(conf)

    // Cleanup output dir
    val hadoopConf = sc.hadoopConfiguration
    var hdfs = FileSystem.get(hadoopConf)
    try { hdfs.delete(new Path(outputPath), true) } catch { case _: Throwable => {} }

    // Read input file
    def f(s: String) = List(s.split(",")(1) -> s.split(",")(0), s.split(",")(0) -> s.split(",")(1));
    
    val lines = sc.textFile(filePath, sc.defaultParallelism)


    sc.stop
  }
}

